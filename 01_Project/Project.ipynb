{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = np.random.randint(0,len(data))\n",
    "question = data[\"question\"][random_num]\n",
    "text = data[\"support\"][random_num]\n",
    "answer = data[\"correct_answer\"][random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 125 tokens.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(question, text)\n",
    "print(\"The input has a total of {} tokens.\".format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        101\n",
      "what       2,054\n",
      "lengths   10,742\n",
      "are        2,024\n",
      "positive   3,893\n",
      "for        2,005\n",
      "con        9,530\n",
      "##ver      6,299\n",
      "##ging     4,726\n",
      "lens      10,014\n",
      "and        1,998\n",
      "negative   4,997\n",
      "for        2,005\n",
      "diver     17,856\n",
      "##ging     4,726\n",
      "lens      10,014\n",
      "?          1,029\n",
      "[SEP]        102\n",
      "-          1,011\n",
      "for        2,005\n",
      "lenses    15,072\n",
      ",          1,010\n",
      "the        1,996\n",
      "distance   3,292\n",
      "from       2,013\n",
      "the        1,996\n",
      "center     2,415\n",
      "of         1,997\n",
      "the        1,996\n",
      "lens      10,014\n",
      "to         2,000\n",
      "the        1,996\n",
      "focus      3,579\n",
      "is         2,003\n",
      ".          1,012\n",
      "focal     15,918\n",
      "lengths   10,742\n",
      "are        2,024\n",
      "positive   3,893\n",
      "for        2,005\n",
      "con        9,530\n",
      "##ver      6,299\n",
      "##ging     4,726\n",
      "lens      10,014\n",
      "and        1,998\n",
      "negative   4,997\n",
      "for        2,005\n",
      "diver     17,856\n",
      "##ging     4,726\n",
      "lens      10,014\n",
      ".          1,012\n",
      "the        1,996\n",
      "distance   3,292\n",
      "from       2,013\n",
      "the        1,996\n",
      "center     2,415\n",
      "of         1,997\n",
      "the        1,996\n",
      "lens      10,014\n",
      "to         2,000\n",
      "the        1,996\n",
      "object     4,874\n",
      "in         1,999\n",
      "question   3,160\n",
      "is         2,003\n",
      ",          1,010\n",
      "where      2,073\n",
      "distances  12,103\n",
      "to         2,000\n",
      "the        1,996\n",
      "left       2,187\n",
      "of         1,997\n",
      "the        1,996\n",
      "lens      10,014\n",
      "are        2,024\n",
      "positive   3,893\n",
      "in         1,999\n",
      "sign       3,696\n",
      ".          1,012\n",
      "the        1,996\n",
      "distance   3,292\n",
      "from       2,013\n",
      "the        1,996\n",
      "center     2,415\n",
      "of         1,997\n",
      "the        1,996\n",
      "lens      10,014\n",
      "to         2,000\n",
      "the        1,996\n",
      "image      3,746\n",
      "is         2,003\n",
      ".          1,012\n",
      "this       2,023\n",
      "number     2,193\n",
      "is         2,003\n",
      "positive   3,893\n",
      "for        2,005\n",
      "real       2,613\n",
      "images     4,871\n",
      "(          1,006\n",
      "formed     2,719\n",
      "to         2,000\n",
      "the        1,996\n",
      "right      2,157\n",
      "of         1,997\n",
      "the        1,996\n",
      "lens      10,014\n",
      ")          1,007\n",
      ",          1,010\n",
      "and        1,998\n",
      "negative   4,997\n",
      "for        2,005\n",
      "virtual    7,484\n",
      "images     4,871\n",
      "(          1,006\n",
      "formed     2,719\n",
      "to         2,000\n",
      "the        1,996\n",
      "left       2,187\n",
      "of         1,997\n",
      "the        1,996\n",
      "lens      10,014\n",
      ")          1,007\n",
      ".          1,012\n",
      "[SEP]        102\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    print('{:8}{:8,}'.format(token,id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEP token index:  17\n",
      "Number of tokens in segment A:  18\n",
      "Number of tokens in segment B:  107\n"
     ]
    }
   ],
   "source": [
    "#first occurence of [SEP] token\n",
    "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "print(\"SEP token index: \", sep_idx)\n",
    "#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
    "num_seg_a = sep_idx+1\n",
    "print(\"Number of tokens in segment A: \", num_seg_a)\n",
    "#number of tokens in segment B (text)\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "print(\"Number of tokens in segment B: \", num_seg_b)\n",
    "#creating the segment ids\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "#making sure that every input token has a segment id\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
    "output = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "What lengths are positive for converging lens and negative for diverging lens?\n",
      "\n",
      "Answer:\n",
      "Focal lengths.\n"
     ]
    }
   ],
   "source": [
    "#tokens with highest start and end scores\n",
    "answer_start = torch.argmax(output.start_logits)\n",
    "answer_end = torch.argmax(output.end_logits)\n",
    "if answer_end >= answer_start:\n",
    "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "    \n",
    "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
    "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = tokens[answer_start]\n",
    "for i in range(answer_start+1, answer_end+1):\n",
    "    if tokens[i][0:2] == \"##\":\n",
    "        answer += tokens[i][2:]\n",
    "    else:\n",
    "        answer += \" \" + tokens[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question, text):\n",
    "    \n",
    "    #tokenize question and text as a pair\n",
    "    input_ids = tokenizer.encode(question, text)\n",
    "    \n",
    "    #string version of tokenized ids\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    #segment IDs\n",
    "    #first occurence of [SEP] token\n",
    "    sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "    #number of tokens in segment A (question)\n",
    "    num_seg_a = sep_idx+1\n",
    "    #number of tokens in segment B (text)\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    \n",
    "    #list of 0s and 1s for segment embeddings\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    \n",
    "    #model output using input_ids and segment_ids\n",
    "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "    \n",
    "    #reconstructing the answer\n",
    "    answer_start = torch.argmax(output.start_logits)\n",
    "    answer_end = torch.argmax(output.end_logits)\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "                \n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "    \n",
    "    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted answer:\n",
      "Brf and br2\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Please enter your text: \\n\")\n",
    "question = input(\"\\nPlease enter your question: \\n\")\n",
    "question_answer(question, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the two possible reduction products for brf3?\n",
      "redox reaction will occur. The only question is whether lead will be oxidized to Pb(II) or Pb(IV). Because BrF3 is a powerful oxidant and fluorine is able to stabilize high oxidation states of other elements, it is likely that PbF4 will be the product. The two possible reduction products for BrF3 are BrF and Br2. The actual product will likely depend on the ratio of the reactants used. With excess BrF3, we expect the more oxidized product (BrF). With lower ratios of oxidant to lead, we would probably obtain Br2 as the product. Exercise Predict the products of each reaction and write a balanced chemical equation for each reaction.\n",
      "brf and br2\n"
     ]
    }
   ],
   "source": [
    "random_num = np.random.randint(0,len(data))\n",
    "question = data[\"question\"][random_num]\n",
    "text = data[\"support\"][random_num]\n",
    "answer = data[\"correct_answer\"][random_num]\n",
    "print(question)\n",
    "print(text)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
