{"cells":[{"cell_type":"markdown","metadata":{"id":"IRBubL3il3TB"},"source":[]},{"cell_type":"markdown","metadata":{"id":"7EdbVSmE87En"},"source":["# HW1: Dictionary-based Tokenization \n"]},{"cell_type":"markdown","metadata":{"id":"pJJLm1Ub87Et"},"source":["In this exercise, you are to implement a dictionary-based word segmentation algorithm. There are two Python functions that you need to complete: \n","<br>\n","* maximal_matching\n","* backtrack\n","</br>\n","\n","Also, you have to find how to use word_tokenize() in PythaiNLP along with customer_dict by yourselves."]},{"cell_type":"markdown","metadata":{"id":"DF5Pme7CK3YF"},"source":["## Part1) Your Maximal Matching with Your Dictionary"]},{"cell_type":"markdown","metadata":{"id":"xzs0R06q87Et"},"source":["### Create a toy dictionary to test the algorithm\n","\n","This is based on the example shown in the lecture. \n","You will tokenize the following text string: \"ไปหามเหสี!\"\n","The toy dictoionary provided in this exercise includes all the charaters, syllables, and words that appear that the text string."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"pq3W4p3z87Ev"},"outputs":[],"source":["thai_vocab = [\"ไ\",\"ป\",\"ห\",\"า\",\"ม\",\"เ\",\"ห\",\"ส\",\"ี\",\"ไป\",\"หา\",\"หาม\",\"เห\",\"สี\",\"มเหสี\",\"!\"]"]},{"cell_type":"markdown","metadata":{"id":"ZornooGF87Ew"},"source":["### Maximal matching \n","Complete the maximal matching  function below to tokenize the input text\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Ao4d2E3387Ew"},"outputs":[],"source":["# c is the character array\n","from math import inf #infinity\n","def maximal_matching(c):\n","    #Initialize an empty 2D list\n","    d =[[None]*len(c) for _ in range(len(c))]\n","    ####FILL CODE HERE####\n","    for i in range(len(c)):\n","        word = \"\"\n","        Num = 0\n","        if i>0:\n","            Min_of_word = list(map(lambda x: d[x][i-1] , range(i)))\n","            Num = min(Min_of_word)\n","        for k in range(i,len(c)):\n","            word+=c[k]\n","            if word in thai_vocab:\n","                d[i][k] = 1+Num\n","            else : \n","                d[i][k] = inf   \n","\n","\n","    ######################\n","    \n","    return d"]},{"cell_type":"markdown","metadata":{"id":"w7vBXfjM87Ew"},"source":["### Backtracking\n","Complete the backtracking function below to find the tokenzied words.\n","It should return a list containing a pair of the beginning position and the ending position of each word.\n","In this example, it should return: \n","<br>\n","[(0, 1),(2, 3),(4, 8),(9, 9)]\n","<br> \n","#### Each pair contains the position of each word as follows:\n","(0, 1) ไป\n","<br>\n","(2, 3) หา\n","<br>\n","(4, 8) มเหสี\n","<br>\n","(9, 9) !\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"SxNFf1IE87Ex"},"outputs":[],"source":["def backtrack(d):\n","    eow = len(d)-1 # End of Word position\n","    word_pos = [] # Word position\n","    ####FILL CODE HERE####\n","    \n","    ######################\n","    word_pos.reverse()\n","    return word_pos\n"]},{"cell_type":"markdown","metadata":{"id":"q0MJkKsh87Ex"},"source":["### Test your maximal matching algorithm on a toy dictionary\n","\n","Expected output:\n","\n","[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n","<br>\n","[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n","<br>\n","[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n","<br>\n","[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n","<br>\n","[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n","<br>\n","[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n","<br>\n","[None, None, None, None, None, None, 4, inf, inf, inf] ห\n","<br>\n","[None, None, None, None, None, None, None, 4, 4, inf] ส\n","<br>\n","[None, None, None, None, None, None, None, None, 5, inf] ี\n","<br>\n","[None, None, None, None, None, None, None, None, None, 4] !\n","<br>"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"tsmVQIKS87Ey"},"outputs":[{"name":"stdout","output_type":"stream","text":["[inf, 1, inf, 1, inf, inf, inf, inf, inf, inf] ไ\n","[None, inf, inf, inf, inf, inf, inf, inf, inf, inf] ป\n","[None, None, inf, 2, 2, inf, inf, inf, inf, inf] ห\n","[None, None, None, inf, inf, inf, inf, inf, inf, inf] า\n","[None, None, None, None, inf, inf, inf, inf, 2, inf] ม\n","[None, None, None, None, None, inf, 3, inf, inf, inf] เ\n","[None, None, None, None, None, None, inf, inf, inf, inf] ห\n","[None, None, None, None, None, None, None, inf, 4, inf] ส\n","[None, None, None, None, None, None, None, None, inf, inf] ี\n","[None, None, None, None, None, None, None, None, None, 3] !\n"]}],"source":["input_text = \"ไปหามเหสี!\"\n","out = maximal_matching(input_text)\n","for i in range(len(out)):\n","    print(out[i],input_text[i])"]},{"cell_type":"markdown","metadata":{"id":"IVhCMM4d87Ey"},"source":["### Test your backtracking algorithm on a toy dictionary\n","Expected output:\n","<br>\n","ไป|หา|มเหสี|!"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"6Hurbm1f87Ey"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["def print_tokenized_text(d, input_text):\n","    tokenized_text=[]\n","    for pos in backtrack(d):\n","        #print(pos)\n","        tokenized_text.append(input_text[pos[0]:pos[1]+1])\n","\n","    print(\"|\".join(tokenized_text))\n","    \n","print_tokenized_text(out,input_text)"]},{"cell_type":"markdown","metadata":{"id":"57rP9cTU87Ez"},"source":["## Part2) Your Maximal Matching with Real Dictionary"]},{"cell_type":"markdown","metadata":{"id":"V306h7AG87Ez"},"source":["For UNIX-based OS users, the following cell will download a dictionary (it's just a list of thai words). Alternatively, you can download it from this link: https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63,"status":"ok","timestamp":1675821102082,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"EFVR9LO187Ez","outputId":"1f38b1ec-8f67-4afe-d173-921993033fbf","scrolled":true},"outputs":[],"source":["# !wget https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1675821102084,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"nqIQzVgE87E0","outputId":"f38bc3d8-83b8-4485-8bc3-af1504b8fdbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocab size: 62069\n"]}],"source":["with open(\"words_th.txt\",encoding='utf-8-sig') as f:\n","    thai_vocab = f.read().splitlines() \n","print(\"Vocab size:\", len(thai_vocab))\n","thai_vocab.extend([\"ๆ\",\"!\"])"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1675821631374,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"LwtodW_5wDkN","outputId":"1a238d7f-e313-483d-ea2e-93e1b82ed657"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["print(\"ไป\" in thai_vocab)"]},{"cell_type":"markdown","metadata":{"id":"Kpjwzw1w87E0"},"source":["### The output of your maximal matching algoithm on a new dictionary\n","Expected output:\n","<br>\n","[1, 1, 100000, 1, 100000, 100000, 100000, 100000, 100000] ไ\n","<br>\n","[None, 2, 100000, 100000, 100000, 100000, 100000, 100000, 100000] ป\n","<br>\n","[None, None, 2, 2, 2, 100000, 100000, 100000, 100000] ห\n","<br>\n","[None, None, None, 100000, 100000, 100000, 100000, 100000, 100000] า\n","<br>\n","[None, None, None, None, 2, 100000, 100000, 100000, 2] ม\n","<br>\n","[None, None, None, None, None, 100000, 3, 100000, 100000] เ\n","<br>\n","[None, None, None, None, None, None, 100001, 100000, 100000] ห\n","<br>\n","[None, None, None, None, None, None, None, 4, 4] ส\n","<br>\n","[None, None, None, None, None, None, None, None, None] ี"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"lYD5ChIS87E0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[inf, 1, inf, 1, inf, inf, inf, inf, inf] ไ\n","[None, inf, inf, inf, inf, inf, inf, inf, inf] ป\n","[None, None, inf, 2, 2, inf, inf, inf, inf] ห\n","[None, None, None, inf, inf, inf, inf, inf, inf] า\n","[None, None, None, None, inf, inf, inf, inf, 2] ม\n","[None, None, None, None, None, inf, 3, inf, inf] เ\n","[None, None, None, None, None, None, inf, inf, inf] ห\n","[None, None, None, None, None, None, None, inf, 4] ส\n","[None, None, None, None, None, None, None, None, inf] ี\n"]}],"source":["input_text = \"ไปหามเหสี\"\n","out = maximal_matching(input_text)\n","for i in range(len(out)):\n","    print(out[i],input_text[i])"]},{"cell_type":"markdown","metadata":{"id":"BSqLuK7G87E0"},"source":["### Expected tokenized text\n","ไปหา|มเหสี"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"TI077jmy87E0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["print_tokenized_text(out,input_text)"]},{"cell_type":"markdown","metadata":{"id":"VLGgO8PrLSz6"},"source":["## Part3) Maximal Matching from PythaiNLP"]},{"cell_type":"markdown","metadata":{"id":"LrZrzQoXLeUX"},"source":["### Default dictionary\n","\n","Study word_tokenize() from PythaiNLP in the link below.\n","\n","https://thainlp.org/pythainlp/docs/2.0/api/tokenize.html"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"yXxPBOcNLXfm"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wheel in c:\\users\\kla\\anaconda3\\lib\\site-packages (0.37.1)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: pythainlp in c:\\users\\kla\\anaconda3\\lib\\site-packages (3.1.1)Note: you may need to restart the kernel to use updated packages.\n","\n","Requirement already satisfied: requests>=2.22.0 in c:\\users\\kla\\anaconda3\\lib\\site-packages (from pythainlp) (2.27.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kla\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kla\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (1.26.9)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kla\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2021.10.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kla\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2.0.4)\n","Requirement already satisfied: marisa_trie in c:\\users\\kla\\anaconda3\\lib\\site-packages (0.7.8)\n","Requirement already satisfied: setuptools in c:\\users\\kla\\anaconda3\\lib\\site-packages (from marisa_trie) (61.2.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# pip install pythainlp\n","# pip install marisa_trie\n","%pip install wheel\n","%pip install pythainlp\n","%pip install marisa_trie"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"goQE5gFUL4KO"},"outputs":[],"source":["from pythainlp.tokenize import word_tokenize\n","text='นัดกินกันตอนไหนก็ได้ที่สามย่านมิตรทาวน์'\n","\n","####FILL CODE HERE####\n","    \n","######################"]},{"cell_type":"markdown","metadata":{"id":"2SlX5cEBMHPd"},"source":["### Custom dictionary\n","\n","Add 'สามย่านมิตรทาวน์' into dictionary and then tokenize again"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"b4V9TqFaMPAj"},"outputs":[],"source":["####FILL CODE HERE####\n","    \n","######################"]}],"metadata":{"colab":{"collapsed_sections":["DF5Pme7CK3YF","xzs0R06q87Et","ZornooGF87Ew","w7vBXfjM87Ew","q0MJkKsh87Ex","57rP9cTU87Ez","Kpjwzw1w87E0","BSqLuK7G87E0","VLGgO8PrLSz6","LrZrzQoXLeUX","2SlX5cEBMHPd"],"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"929163599431c7b3e753eb9c75a6800fa8c5c72fb3910ee126ea89c879162a29"}}},"nbformat":4,"nbformat_minor":0}
